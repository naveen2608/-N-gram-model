# Importing the required libraries
import pandas as pd
import re
import random
import tarfile
import os
import math
from tqdm import tqdm
import secrets

# Connecting google drive with notebook
from google.colab import drive
drive.mount('/content/drive')

# Extracting data from the tar file
fname = "/content/drive/MyDrive/nlp dataset/aclImdb_v1.tar.gz"
tar = tarfile.open(fname, "r:gz")
tar.extractall('.')
tar.close()

# Creating corpus from downloaded data
reviews = []
def adder(file):
    for i in os.listdir(file):
        fname = os.path.join(file, i)
        f = open(fname, 'r')
        reviews.append(f.read().split("."))

adder("/content/aclImdb/train/pos")
adder("/content/aclImdb/train/neg")

print(f"Total reviews: {len(reviews)}")

# Size of the corpus
data = []
for i in reviews:
    for j in i:
        if j != "":
            data.append(j)
print(f"Total sentences: {len(data)}")

# Removing punctuations and additional html tags from the corpus
reg = "<br /><br />|\'s|\.\.+|\!|\@|\#|\$|\%|\^|\&|\*|\(|\)|\_|\-|\=|\+|\}|\{|\[|\]|\:|\;|\'|\"|\>|\<|\,|\/|\`|\~"
def tagremover(sent):
    sent = re.sub(reg, " ", sent)
    return sent

for i in range(len(data)):
    data[i] = tagremover(data[i]).lower().strip()

print("Data cleaning completed!")

# Creating N-gram model
class N_gram:
    def __init__(self, data, n):
        self.data = data
        self.n = n
    
    def gramDiv(self):
        gram_count = dict()
        hist_count = dict()
        sent_data = []
        
        if self.n == 1:
            # Unigram case
            for i in tqdm(range(len(self.data)), desc="Processing unigrams"):
                sent = "<s> " + self.data[i] + " </s>"
                sent_data.append(sent)
                sentence = sent.split()
                
                for word in sentence:
                    gram_count[(word,)] = gram_count.get((word,), 0) + 1
                    hist_count[('',)] = hist_count.get(('',), 0) + 1
        else:
            # N-gram case (bigram, trigram, etc.)
            for i in tqdm(range(len(self.data)), desc=f"Processing {self.n}-grams"):
                sent = "<s> " * (self.n - 1) + self.data[i] + " </s>"
                sent_data.append(sent)
                
                sentence = sent.split()
                given_n_word = sentence[:self.n - 1]
                
                for j in range(self.n - 1, len(sentence)):
                    current_word = sentence[j]
                    gram_count[(tuple(given_n_word), current_word)] = gram_count.get((tuple(given_n_word), current_word), 0) + 1
                    hist_count[tuple(given_n_word)] = hist_count.get(tuple(given_n_word), 0) + 1
                    given_n_word.pop(0)
                    given_n_word.append(current_word)
        
        return gram_count, hist_count, sent_data

# Get n value from user
n = int(input("Enter n-value (1 for unigram, 2 for bigram, 3 for trigram, etc.): "))
obj = N_gram(data, n)
gram_n1_count, gram_n0_count, sent_data = obj.gramDiv()

# Calculate vocabulary size (corrected)
def word_count_in_corpus(sent_data):
    word_count_corpus = dict()
    for i in tqdm(range(len(sent_data)), desc="Counting vocabulary"):
        for j in sent_data[i].split():
            word_count_corpus[j] = word_count_corpus.get(j, 0) + 1
    return word_count_corpus

word_count_corpus = word_count_in_corpus(sent_data)
vocabulary_size = len(word_count_corpus)  # CORRECTED: Just count unique words
print(f"Vocabulary size: {vocabulary_size}")

# Finding probabilities
def calculate_prob(gram_n0_count, gram_n1_count):
    ngram_probs = dict()
    for ngram, count in gram_n1_count.items():
        if n == 1:
            history = ('',)
        else:
            history = ngram[0]
        numerator = count
        denominator = gram_n0_count.get(history, 0)
        if denominator > 0:
            ngram_probs[ngram] = numerator / denominator
    return ngram_probs

ngram_probs = calculate_prob(gram_n0_count, gram_n1_count)
print(f"Total n-gram probabilities calculated: {len(ngram_probs)}")

# Load test data
test_data = []
def adder_test(file):
    for i in os.listdir(file):
        fname = os.path.join(file, i)
        f = open(fname, 'r')
        test_data.append(f.read().split("."))

adder_test("/content/aclImdb/test/neg")
print(f"Test data loaded: {len(test_data)} reviews")

# Select random test sentence
test_sentence = secrets.choice(secrets.choice(test_data))
test_sentence = tagremover(test_sentence).lower().strip()
print(f"\nTest sentence: {test_sentence}")

# Finding n-grams for test sentence
def prob_for_test_sent(test_sentence, n):
    list_of_ngrams_of_sent = []
    sent = "<s> " * (n - 1) + test_sentence + " </s>"
    sentence_list = sent.split()
    
    if n == 1:
        for word in sentence_list:
            list_of_ngrams_of_sent.append((('',), word))
    else:
        given_n_word = sentence_list[:n - 1]
        for j in range(n - 1, len(sentence_list)):
            current_word = sentence_list[j]
            list_of_ngrams_of_sent.append((tuple(given_n_word), current_word))
            given_n_word.pop(0)
            given_n_word.append(current_word)
    
    return list_of_ngrams_of_sent, sentence_list

list_of_ngrams_of_sent, sentence_list = prob_for_test_sent(test_sentence, n)

# CORRECTED: Perplexity calculation with proper formula
def perplexity_of_test_sent(list_of_ngrams_of_sent, n, vocabulary_size):
    log_prob_sum = 0  # Use log to avoid underflow
    
    for j in list_of_ngrams_of_sent:
        given_n_word = j[0]
        current_word = j[1]
        
        # Add-1 (Laplace) smoothing - CORRECTED with if-else
        if (given_n_word, current_word) in gram_n1_count:
            numerator = gram_n1_count[(given_n_word, current_word)] + 1
        else:
            numerator = 1
        
        if given_n_word in gram_n0_count:
            denominator = gram_n0_count[given_n_word] + vocabulary_size
        else:
            denominator = vocabulary_size
        
        probability = numerator / denominator
        log_prob_sum += math.log(probability)
    
    # CORRECTED: Perplexity = exp(-1/N * log(P(sentence)))
    perplexity = math.exp(-log_prob_sum / len(list_of_ngrams_of_sent))
    
    # Also calculate sentence probability
    sent_prob = math.exp(log_prob_sum)
    
    return perplexity, sent_prob

perplexity, sent_prob = perplexity_of_test_sent(list_of_ngrams_of_sent, n, vocabulary_size)

print("\n" + "="*50)
print("RESULTS")
print("="*50)
print(f"Sentence probability: {sent_prob:.2e}")
print(f"Perplexity of the test sentence: {perplexity:.4f}")
print("="*50)

# BONUS: Next word prediction function
def predict_next_word(history, ngram_probs, top_k=5):
    """
    Predict the next word given a history
    history: tuple of previous words (for bigram: 1 word, trigram: 2 words, etc.)
    """
    candidates = {}
    
    # Convert history string to tuple if needed
    if isinstance(history, str):
        history = tuple(history.lower().split())
    
    # Find all possible next words
    for ngram, prob in ngram_probs.items():
        if n == 1:
            # For unigram, just return most common words
            candidates[ngram[0]] = prob
        else:
            # For n-grams, match the history
            if ngram[0] == history:
                candidates[ngram[1]] = prob
    
    # Sort by probability and return top k
    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:top_k]
    return sorted_candidates

# Example usage of next word prediction
if n > 1:
    print("\n" + "="*50)
    print("NEXT WORD PREDICTION EXAMPLE")
    print("="*50)
    # Get the first few words from test sentence as history
    example_history = tuple(sentence_list[1:n])
    print(f"Given history: {' '.join(example_history)}")
    predictions = predict_next_word(example_history, ngram_probs, top_k=5)
    print("\nTop 5 predicted next words:")
    for word, prob in predictions:
        print(f"  {word}: {prob:.6f}")
    print("="*50)
